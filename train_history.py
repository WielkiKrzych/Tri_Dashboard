import os
import glob
import pandas as pd
import numpy as np
import time
import json

# --- MLX SETUP ---
try:
    import mlx.core as mx
    import mlx.nn as nn
    import mlx.optimizers as optim
    print("âœ… Wykryto Apple Silicon (MLX). Jedziemy z koksem.")
except ImportError:
    print("âŒ BÅÄ„D: Brak biblioteki MLX. Zainstaluj: pip install mlx")
    exit()

# KONFIGURACJA
DATA_FOLDER = "treningi_csv"  # Tutaj wrzuÄ‡ pliki CSV lub JSON
MODEL_FILE = "cycling_brain_weights.npz"
HISTORY_FILE = "brain_evolution_history.json"

# --- DEFINICJA MODELU (Musi byÄ‡ identyczna jak w app.py) ---
class PhysioNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.Linear(3, 64)
        self.l2 = nn.Linear(64, 64)
        self.l3 = nn.Linear(64, 1)

    def __call__(self, x):
        x = nn.relu(self.l1(x))
        x = nn.relu(self.l2(x))
        return self.l3(x)

# --- FUNKCJE POMOCNICZE ---

def load_data(filepath):
    """Smart Loader: Radzi sobie z zagnieÅ¼dÅ¼onymi JSONami i CSV"""
    file_ext = os.path.splitext(filepath)[1].lower()
    filename = os.path.basename(filepath)
    
    try:
        if file_ext == '.json':
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            # PRZYPADEK 1: JSON to po prostu lista rekordÃ³w (Idealnie)
            if isinstance(data, list):
                df = pd.DataFrame(data)
                
            # PRZYPADEK 2: JSON to sÅ‚ownik (Dict)
            elif isinstance(data, dict):
                # Szukamy klucza, ktÃ³ry trzyma "miÄ™so" (najdÅ‚uÅ¼szÄ… listÄ™)
                # CzÄ™ste nazwy w apkach sportowych:
                candidates = ['samples', 'data', 'records', 'trackPoints', 'points', 'streams', 'rows']
                
                target_list = None
                
                # A. Szukamy po znanych nazwach
                for key in candidates:
                    if key in data and isinstance(data[key], list):
                        target_list = data[key]
                        break
                
                # B. JeÅ›li nie znaleziono po nazwie, szukamy NAJDÅUÅ»SZEJ listy w caÅ‚ym JSON-ie
                if target_list is None:
                    max_len = 0
                    for k, v in data.items():
                        if isinstance(v, list) and len(v) > max_len:
                            # Dodatkowe zabezpieczenie: lista musi zawieraÄ‡ sÅ‚owniki (rekordy)
                            if len(v) > 0 and isinstance(v[0], dict):
                                target_list = v
                                max_len = len(v)
                
                # C. JeÅ›li nadal nic, moÅ¼e to format kolumnowy? {'time': [1,2], 'watts': [100, 200]}
                # PrÃ³bujemy stworzyÄ‡ DF bezpoÅ›rednio, ale bezpiecznie
                if target_list is None:
                    try:
                        # dict_of_lists
                        df = pd.DataFrame.from_dict(data, orient='columns')
                        # JeÅ›li zadziaÅ‚aÅ‚o, ale kolumny majÄ… rÃ³Å¼ne dÅ‚ugoÅ›ci, Pandas rzuci bÅ‚Ä…d, ktÃ³ry zÅ‚apiemy niÅ¼ej
                    except ValueError:
                        # Ostatnia deska ratunku: json_normalize na pÅ‚asko
                        df = pd.json_normalize(data)
                else:
                    # Mamy naszÄ… listÄ™!
                    df = pd.json_normalize(target_list)

        else:
            # Åadowanie CSV / TXT (Stara metoda)
            try:
                df = pd.read_csv(filepath, low_memory=False)
            except:
                df = pd.read_csv(filepath, sep=';', low_memory=False)
    
        # --- CZYSZCZENIE FINALNE ---
        if 'df' in locals() and not df.empty:
            # Normalizacja nazw kolumn (maÅ‚e litery, bez spacji)
            df.columns = [str(c).lower().strip() for c in df.columns]
            
            # Fix dla TymeWear / json nested (czasami dane sÄ… w 'data.watts', 'data.time')
            # Usuwamy prefixy typu 'data.' z nazw kolumn
            df.columns = [c.split('.')[-1] for c in df.columns]
            
            return df
        else:
            print(f"   -> âš ï¸ Pusty lub nieczytelny plik: {filename}")
            return pd.DataFrame()

    except Exception as e:
        print(f"   -> âš ï¸ Krytyczny bÅ‚Ä…d odczytu {filename}: {e}")
        return pd.DataFrame()

def process_data(df):
    if df.empty: return df

    # Minimalna obrÃ³bka potrzebna do treningu
    if 'time' not in df.columns:
        # JeÅ›li brak czasu, tworzymy sztuczny
        df['time'] = np.arange(len(df)).astype(float)
    
    # Sortowanie i czyszczenie
    df = df.sort_values('time').reset_index(drop=True)
    
    # Konwersja kolumn numerycznych (dla bezpieczeÅ„stwa, zwÅ‚aszcza przy JSON)
    cols_to_numeric = ['watts', 'heartrate', 'cadence', 'time']
    for c in cols_to_numeric:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors='coerce')

    # UzupeÅ‚nianie dziur (Interpolacja)
    num_cols = df.select_dtypes(include=['float64', 'int64']).columns
    if len(num_cols) > 0:
        df[num_cols] = df[num_cols].interpolate(method='linear').ffill().bfill()

    # Smoothing (kluczowe dla sieci neuronowej)
    window = 30 # 30 sekund
    if 'watts' in df.columns:
        df['watts_smooth'] = df['watts'].rolling(window=window, min_periods=1).mean()
    if 'heartrate' in df.columns:
        df['heartrate_smooth'] = df['heartrate'].rolling(window=window, min_periods=1).mean()
    if 'cadence' in df.columns:
        df['cadence_smooth'] = df['cadence'].rolling(window=window, min_periods=1).mean()
    
    df['time_min'] = df['time'] / 60.0
    return df

def filter_and_prepare(df, target_watts, tolerance=15, min_samples=30):
    """
    Filtruje dane tylko dla konkretnego zakresu mocy.
    Np. dla 280W bierze zakres 265W-295W.
    """
    if df.empty or 'watts_smooth' not in df.columns:
        return None, None

    # Maska: szukamy momentÃ³w, gdzie moc byÅ‚a blisko celu
    mask = (df['watts_smooth'] >= target_watts - tolerance) & \
           (df['watts_smooth'] <= target_watts + tolerance)
    
    # JeÅ›li mamy za maÅ‚o danych (np. mniej niÅ¼ 30 sekund w strefie), odpuszczamy
    if mask.sum() < min_samples:
        return None, None

    df_filtered = df[mask].copy()

    # Przygotowanie TensorÃ³w MLX
    w = df_filtered['watts_smooth'].values / 500.0
    c = df_filtered['cadence_smooth'].values / 120.0 if 'cadence_smooth' in df_filtered else np.zeros_like(w)
    t = df_filtered['time_min'].values / df['time_min'].max() # Normalizacja czasem caÅ‚ego treningu
    y_target = df_filtered['heartrate_smooth'].values / 200.0

    X_np = np.column_stack((w, c, t)).astype(np.float32)
    y_np = y_target.astype(np.float32).reshape(-1, 1)

    return mx.array(X_np), mx.array(y_np)

def update_history(hr_base, hr_thresh, filename):
    history = []
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r') as f:
                history = json.load(f)
        except: pass
    
    entry = {
        "timestamp": time.time(),
        "date": time.strftime("%Y-%m-%d %H:%M"),
        "source_file": filename,
        # Zapisujemy None, jeÅ›li brak danych (JSON to przyjmie jako null)
        "hr_base": float(hr_base) if hr_base is not None else None,
        "hr_thresh": float(hr_thresh) if hr_thresh is not None else None
    }
    history.append(entry)
    
    with open(HISTORY_FILE, 'w') as f:
        json.dump(history, f)

def train_loop():
    # 1. Szukamy plikÃ³w
    files = glob.glob(os.path.join(DATA_FOLDER, "*.csv"))
    files += glob.glob(os.path.join(DATA_FOLDER, "*.txt"))
    files += glob.glob(os.path.join(DATA_FOLDER, "*.json"))
    
    if not files:
        print(f"âš ï¸ Nie znaleziono plikÃ³w w folderze '{DATA_FOLDER}'.")
        return

    print(f"ðŸ“‚ Znaleziono {len(files)} treningÃ³w. SortujÄ™ chronologicznie...")
    files.sort() 

    # Inicjalizacja modelu (Globalnego)
    model = PhysioNet()
    mx.eval(model.parameters())
    
    # Funkcja straty i optimizer
    def mse_loss(pred, target): return mx.mean((pred - target) ** 2)
    optimizer = optim.Adam(learning_rate=0.02) # Nieco agresywniejsze uczenie dla wycinkÃ³w
    
    def train_step(model, X, y):
        pred = model(X)
        loss = mse_loss(pred, y)
        return loss

    loss_and_grad_fn = nn.value_and_grad(model, train_step)
    
    targets = {
        "BASE": 280,   # Cel 1: Baza
        "THRESH": 360  # Cel 2: PrÃ³g
    }

    total_start = time.time()
    
    for idx, file_path in enumerate(files):
        filename = os.path.basename(file_path)
        print(f"\n[{idx+1}/{len(files)}] Analiza: {filename}")
        
        try:
            df_raw = load_data(file_path)
            if df_raw.empty: continue

            df = process_data(df_raw)
            if len(df) < 100: continue

            results = {} # Tu zbierzemy wyniki dla tego pliku

            # --- NOWA LOGIKA: Trenujemy osobno dla kaÅ¼dego celu ---
            for name, watts in targets.items():
                
                # 1. Filtrujemy dane (tylko momenty, gdzie jechaÅ‚eÅ› ~Watts)
                X_chunk, y_chunk = filter_and_prepare(df, watts, tolerance=15, min_samples=60)
                
                if X_chunk is not None:
                    # Resetujemy wagi modelu do stanu globalnego (lub trenujemy dalej - tu decydujemy siÄ™ na fine-tuning)
                    # W tym skrypcie robimy fine-tuning ciÄ…gÅ‚y, ale na przefiltrowanych danych model "przypomni sobie" konkretnÄ… strefÄ™
                    
                    # Szybki trening na tym wycinku (Overfitting jest tu poÅ¼Ä…dany, bo chcemy odwzorowaÄ‡ TEN trening)
                    for _ in range(100): 
                        loss, grads = loss_and_grad_fn(model, X_chunk, y_chunk)
                        optimizer.update(model, grads)
                        mx.eval(model.parameters(), optimizer.state)
                    
                    # Predykcja
                    # Parametry wejÅ›ciowe: [Moc, Kadencja, Czas(poÅ‚owa treningu)]
                    # KadencjÄ™ przyjmujemy optymalnÄ… (85-90) lub Å›redniÄ… z wycinka
                    cadence_norm = 90.0/120.0 
                    in_tensor = mx.array([[watts/500.0, cadence_norm, 0.5]])
                    
                    pred_hr = float(model(in_tensor)[0][0]) * 200.0
                    results[name] = pred_hr
                    print(f"   -> {name} ({watts}W): {pred_hr:.1f} bpm (znaleziono dane)")
                
                else:
                    results[name] = None
                    print(f"   -> {name} ({watts}W): Brak danych w tym treningu.")

            # Zapisujemy wynik do historii
            update_history(results["BASE"], results["THRESH"], filename)

        except Exception as e:
            print(f"   -> ðŸ’¥ BÅ‚Ä…d: {e}")

    print("-" * 30)
    print("Zapisano historiÄ™ ewolucji formy.")

    # SpÅ‚aszczanie i zapis
    params = model.parameters()
    flat_params = {}
    for layer_name, layer_params in params.items():
        if isinstance(layer_params, dict):
            for param_name, param_value in layer_params.items():
                flat_params[f"{layer_name}.{param_name}"] = param_value
        else:
            flat_params[layer_name] = layer_params
    mx.savez(MODEL_FILE, **flat_params)
    
    total_time = time.time() - total_start
    print(f"ðŸš€ GOTOWE! Przemielono {len(files)} plikÃ³w w {total_time:.1f} sekund.")

if __name__ == "__main__":
    train_loop()